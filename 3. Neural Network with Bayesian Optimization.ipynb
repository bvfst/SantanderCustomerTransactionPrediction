{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6ddee1",
   "metadata": {},
   "source": [
    "# Neural Networks with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9fd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317fbdc",
   "metadata": {},
   "source": [
    "## Read the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62bcc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db00db",
   "metadata": {},
   "source": [
    "## Identifying the real and synthetic records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de50ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real samples in test set is 100000\n",
      "Number of synthetic samples in test set is 100000\n"
     ]
    }
   ],
   "source": [
    "test = df_test.drop(['ID_code'], axis=1).values\n",
    "\n",
    "unique_count = np.zeros_like(test)\n",
    "\n",
    "for feature in range(test.shape[1]):\n",
    "    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index[count == 1], feature] += 1\n",
    "    \n",
    "real_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "print('Number of real samples in test set is {}'.format(len(real_samples)))\n",
    "print('Number of synthetic samples in test set is {}'.format(len(synth_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c611b87",
   "metadata": {},
   "source": [
    "## Magic Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026cc36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[feature + 'sum'] = ((df_train[feature] - df_all[feature].mean()) * df_train[feature + 'vc'] \\\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
      "C:\\Users\\YASHIS~1\\AppData\\Local\\Temp/ipykernel_8264/362531156.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[feature + 'sum'] = ((df_test[feature] - df_all[feature].mean()) * df_test[feature + 'vc'] \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape after creating magic features: (200000, 1002)\n",
      "Test set shape after creating magic features: (200000, 1001)\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in df_train.columns if col.startswith('var')]\n",
    "df_all = pd.concat([df_train, df_test.iloc[real_samples]])\n",
    "\n",
    "for feature in features:\n",
    "    temp = df_all[feature].value_counts(dropna=True)\n",
    "\n",
    "    df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
    "    df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
    "\n",
    "    df_train[feature + 'sum'] = ((df_train[feature] - df_all[feature].mean()) * df_train[feature + 'vc'] \\\n",
    "                                 .map(lambda x: int(x > 1))).astype(np.float32)\n",
    "    df_test[feature + 'sum'] = ((df_test[feature] - df_all[feature].mean()) * df_test[feature + 'vc'] \\\n",
    "                                .map(lambda x: int(x > 1))).astype(np.float32) \n",
    "\n",
    "    df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n",
    "                                  .map(lambda x: int(x > 2))).astype(np.float32)\n",
    "    df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n",
    "                                 .map(lambda x: int(x > 2))).astype(np.float32)\n",
    "\n",
    "    df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'] \\\n",
    "                                  .map(lambda x: int(x > 4))).astype(np.float32) \n",
    "    df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'] \\\n",
    "                                 .map(lambda x: int(x > 4))).astype(np.float32)\n",
    "    \n",
    "print('Training set shape after creating magic features: {}'.format(df_train.shape))\n",
    "print('Test set shape after creating magic features: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6139020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the predictor and target variables \n",
    "x = df_train.iloc[:,2:].values\n",
    "y = df_train.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a22df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Class Imbalance\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# # sm = SMOTE(random_state=42)\n",
    "# # x, y = sm.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ab5f0",
   "metadata": {},
   "source": [
    "## Test and train split for the model fit and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13756144",
   "metadata": {
    "_uuid": "cad492af56af9b03b012e97b9ad9033799f0c46c"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc998b23",
   "metadata": {},
   "source": [
    "## Metrics shown during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6eba4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c90bb5",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for optimal hyper parameters using Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6247fa",
   "metadata": {},
   "source": [
    "KerasTuner is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search. KerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms.\n",
    "\n",
    "For the current project we've used Bayesian Optimization for hyper parameter tuning.\n",
    "\n",
    "Reference: https://keras.io/keras_tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604eb38e",
   "metadata": {},
   "source": [
    "### Building a base model with parameter search space\n",
    "\n",
    "The function that creates and returns a Keras model. Use the hp argument to define the hyperparameters during model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2969369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1c5aa0e5fd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=METRICS, # [\"val_accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2d0d5",
   "metadata": {},
   "source": [
    "### Defining the tuner which will use bayesian optimization to find the optimal hyper parameters from the pre defined search space\n",
    "\n",
    "Tuner is initialized here. We use objective to specify the objective to select the best models, and we use max_trials to specify the number of different models to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88e735ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel = build_model,\n",
    "    objective = kt.Objective(\"auc\", direction=\"max\"), # 'val_accuracy'\n",
    "    max_trials = 5,\n",
    "    num_initial_points=2,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=42,\n",
    "    hyperparameters=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True\n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6f44f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 46s]\n",
      "auc: 0.8594379425048828\n",
      "\n",
      "Best auc So Far: 0.8604117035865784\n",
      "Total elapsed time: 00h 06m 37s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train, epochs=2, validation_data=(X_test, y_test))\n",
    "# tuner.search(X_res, y_res, epochs=2, validation_data=(X_val, y_val)) # With SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97d0b5",
   "metadata": {},
   "source": [
    "### Get the best hyperparameters after the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0baa3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 2, 'units_0': 288, 'activation': 'relu', 'dropout': False, 'lr': 0.0003069273045576997, 'units_1': 512}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9440ce",
   "metadata": {},
   "source": [
    "### Use the above optimal hyperparameters to build the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a14fe37",
   "metadata": {
    "_uuid": "e2e99761450bf278facc73b2c964aa9a22c83db2"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(288,kernel_initializer='normal', activation='relu', input_dim=1000))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation= 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Fit the model on train set\n",
    "model.fit(X_train, Y_train, epochs=5) # validation_data=(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f493cc",
   "metadata": {},
   "source": [
    "### Model evaluation on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc1ddff",
   "metadata": {
    "_uuid": "c60e680888727afb3bf5424718952d759abeb225"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "metrics.roc_auc_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d8e79",
   "metadata": {},
   "source": [
    "## Model prediction on Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9ae214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_197sum2</th>\n",
       "      <th>var_197sum3</th>\n",
       "      <th>var_198vc</th>\n",
       "      <th>var_198sum</th>\n",
       "      <th>var_198sum2</th>\n",
       "      <th>var_198sum3</th>\n",
       "      <th>var_199vc</th>\n",
       "      <th>var_199sum</th>\n",
       "      <th>var_199sum2</th>\n",
       "      <th>var_199sum3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.402238</td>\n",
       "      <td>15.472200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.420920</td>\n",
       "      <td>-8.7197</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>3</td>\n",
       "      <td>3.254862</td>\n",
       "      <td>19.129299</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.677219</td>\n",
       "      <td>-20.9760</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>4.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.857638</td>\n",
       "      <td>13.016800</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.948438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.885820</td>\n",
       "      <td>-9.1846</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8  ...  var_197sum2  var_197sum3  var_198vc  var_198sum  \\\n",
       "0  18.2675  2.1337  ...      10.7200       0.0000          4   -0.402238   \n",
       "1  18.6316 -4.4131  ...       9.8714       9.8714          3    3.254862   \n",
       "2  20.2537  1.5233  ...       7.0618       0.0000          2    4.021162   \n",
       "3  20.5660  3.3755  ...       9.2295       9.2295          7   -2.857638   \n",
       "4  10.6048  2.9890  ...       7.2882       0.0000          2   -1.948438   \n",
       "\n",
       "   var_198sum2  var_198sum3  var_199vc  var_199sum  var_199sum2  var_199sum3  \n",
       "0    15.472200       0.0000          4   -5.420920      -8.7197         -0.0  \n",
       "1    19.129299       0.0000          3  -17.677219     -20.9760         -0.0  \n",
       "2     0.000000       0.0000          1   -0.000000      -0.0000         -0.0  \n",
       "3    13.016800      13.0168          1   -0.000000      -0.0000         -0.0  \n",
       "4     0.000000       0.0000          3   -5.885820      -9.1846         -0.0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaggle test data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "761cb226",
   "metadata": {
    "_uuid": "cab78c6d3e3ac7307501be87244e2405bb865589"
   },
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:, 1:].values\n",
    "Y_pred = model.predict(x_test)\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['target'] = Y_pred\n",
    "sub.to_csv('submission_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
